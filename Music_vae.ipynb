{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhrfhIp72dmQ+BADsCDIjn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddoubleseat/MusicVAE/blob/main/Music_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music**"
      ],
      "metadata": {
        "id": "e0wVZ4yKo6tR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  기존의 VAE(Variational Autoencoder)를 개선 시킨것이 MusicVAE라고 할 수 있다. 그렇다면 Variational은 무엇이고 Autoencoder란 무엇일까? 우선 Autoencoder가 무엇인지 알아보자."
      ],
      "metadata": {
        "id": "yIWlC3QWpU7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder vs Variational Autoencoder"
      ],
      "metadata": {
        "id": "51v-SFQFvXci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Autoencoder는 비지도 unsupervised 방식으로 훈련된 인공 신경망으로, 먼저 데이터에 인코딩 된 표현을 학습한 다음, 학습 된 인코딩 표현에서 입력 데이터를 (가능한한 가깝게) 생성하는 것을 목표로 한다. 따라서, 오토인코더의 출력은 입력에 대한 예측이다. 그렇다면 Variational Autoencoder란 무엇일까.  \n",
        "AE와 VAE의 가장 큰 차이점은 AE는 Latent space z에 값을 저장하고 VAE는 확률분포를 저장하여 파라미터를 생성한다는 점이라고 한다. 즉 AE는 input x 자신을 언제든지 reconstruct할 수 있는 z를 만드는것이 목적이고 VAE는 input x가 만들어지는 확률 분포를 찾는것이 목적이다."
      ],
      "metadata": {
        "id": "WIjhxXxPrM6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MusicVAE / A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music"
      ],
      "metadata": {
        "id": "-x1zAd6SvgMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VAE는 자연 데이터로부터 의미있는 결과를 도출하는 모델임이 분명했지만, sequential data적용에는 제한이 있었고 기존의 순환 VAE 모델은 음악과 같이 long-term structured sequence를 모델링하는데 어려움이 있었다고 한다. 첫번째로 RNN은 강력한 자기 회귀 모델이기 떄문에 RNN 디코더는 데이터를 학습하기 위해 Latent를 무시할 수 있기 때문이고 두번째는 Autoencoding은 전체 시퀀스를 하나의 Latent Vector로 압축해야 했기 때문에 짧은 시퀀스에는 잘 작동하지만 길이기 길어질 경우에는 성능이 현저히 떨어졌기 때문이다.  \n",
        "따라서 이 문제를 해결하기 위해 MusicVAE에서는 Hierarchical RNN을 디코더에 사용한다. 이 구조는 모델이 Latent Code를 좀 더 활용하도록하여 recurrent VAE의 고질적인 문제인 'posterior collapse(사후 붕괴)'를 피할 수 있도록 한다.  \n",
        "ResNet 모델에서 Vanishing Gradient 문제를 회피하기 위해 Residual Block 개념을 도입한것과 유사하다는 생각이 든다."
      ],
      "metadata": {
        "id": "xG_yPNAnrBwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mission"
      ],
      "metadata": {
        "id": "vHzxNVuufYxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Music VAE, Groove MIDI Dataset을 이용하여 4마디에 해당하는 드럼 샘플을 뽑아내는 과정을 수행*"
      ],
      "metadata": {
        "id": "2tA3_5flnkIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "TjX-1Aw8aM3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab 사용시\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgpKzwR9aL7K",
        "outputId": "86eaaa64-4370-4b97-d4b0-a161550b9976"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install"
      ],
      "metadata": {
        "id": "palif6EPhasc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt-get update -qq && apt-get install -qq libfluidsynth2 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n",
        "# !pip install -q pyfluidsynth\n",
        "# !pip install -U -q magenta"
      ],
      "metadata": {
        "id": "44cAt231hank"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Library"
      ],
      "metadata": {
        "id": "l6jT3dQBhabb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 환경 설정\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tf_slim\n",
        "\n",
        "# Colab/Notebook specific stuff\n",
        "# import IPython.display\n",
        "# from IPython.display import Audio\n",
        "# from google.colab import files\n",
        "\n",
        "# Magenta specific stuff\n",
        "from magenta.common import merge_hparams\n",
        "from magenta.contrib import training as contrib_training\n",
        "from magenta.models.music_vae import configs\n",
        "from magenta.models.music_vae import data\n",
        "from magenta.models.music_vae import data_hierarchical\n",
        "from magenta.models.music_vae import lstm_models\n",
        "from magenta.models.music_vae.base_model import MusicVAE\n",
        "from magenta.models.music_vae.trained_model import TrainedModel\n",
        "from magenta.scripts.convert_dir_to_note_sequences import convert_directory\n",
        "\n",
        "import note_seq\n",
        "from note_seq import midi_synth\n",
        "from note_seq.sequences_lib import concatenate_sequences\n",
        "from note_seq.protobuf import music_pb2\n",
        "\n",
        "HParams = contrib_training.HParams"
      ],
      "metadata": {
        "id": "Hzaplsx-hdjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aabe94e1-2b05-4c4c-a1d2-0c0162d2cb42"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "/usr/local/lib/python3.8/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper_Parameters"
      ],
      "metadata": {
        "id": "HfZhdUzlhh4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab directory\n",
        "DATA_DIR = '/content/drive/MyDrive/music_vae/Dataset/groove-v1.0.0-midionly/groove/'\n",
        "TFRECORD_DIR = '/content/drive/MyDrive/music_vae/Tfrecord/music.tfrecord'\n",
        "TRAIN_DIR = '/content/drive/MyDrive/music_vae/Train/'\n",
        "\n",
        "# HParams\n",
        "BATCH_SIZE=512 # batch size\n",
        "MAX_SEQ_LEN=16 * 4  # 4 bars w/ 16 steps per bar, 시퀀스 길이 제한\n",
        "Z_SIZE=256 # Latent Vector\n",
        "ENC_RNN_SIZE=[512] # Encoder RNN SIZE\n",
        "DEC_RNN_SIZE=[256, 256] # Decoder RNN SIZE\n",
        "MAX_BETA=0.2\n",
        "FREE_BITS=48\n",
        "DROPOUT_KEEP_PROB=0.3 # Drop out\n",
        "\n",
        "NUM_STEPS = 100 # Epoch"
      ],
      "metadata": {
        "id": "Cr1BlUDHhhzE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Preprocessing"
      ],
      "metadata": {
        "id": "0eFPlFzKbP1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "convert_directory(DATA_DIR,TFRECORD_DIR,recursive=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpdRKVRhbP6C",
        "outputId": "d4d715ba-7206-4f69-d1e3-63406b848983"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/music_vae/Dataset/groove-v1.0.0-midionly/groove/LICENSE\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/music_vae/Dataset/groove-v1.0.0-midionly/groove/README\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/music_vae/Dataset/groove-v1.0.0-midionly/groove/info.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Set Config"
      ],
      "metadata": {
        "id": "UjNIPnp31796"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "configuratoin 설정(From configs.py)"
      ],
      "metadata": {
        "id": "DuiCe5nUmKzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Config 클래스\n",
        "class Config(collections.namedtuple(\n",
        "    'Config',\n",
        "    ['model', 'hparams', 'note_sequence_augmenter', 'data_converter',\n",
        "     'train_examples_path', 'eval_examples_path', 'tfds_name'])):\n",
        "\n",
        "  def values(self):\n",
        "    return self._asdict()\n",
        "\n",
        "Config.__new__.__defaults__ = (None,) * len(Config._fields)\n",
        "\n",
        "def update_config(config, update_dict):\n",
        "  config_dict = config.values()\n",
        "  config_dict.update(update_dict)\n",
        "  return Config(**config_dict)\n",
        "\n",
        "CONFIG_MAP = {}\n",
        "\n",
        "# GrooVAE config\n",
        "# 드럼 4마디를 생성하기 위한 모델의 config\n",
        "CONFIG_MAP['groovae_4bar'] = Config(\n",
        "    model=MusicVAE(lstm_models.BidirectionalLstmEncoder(),\n",
        "                   lstm_models.GrooveLstmDecoder()),\n",
        "    # 3.Model\n",
        "    # 3.1 Bidirectional Encoder - Encoder에 Bi-LSTM(양방향 LSTM)신경망 적용\n",
        "    # RNN이나 LSTM은 입력 순서를 시간 순대로 입력하기 때문에 결과물이 직전 패턴을 기반으로 수렴하는 경향을 보인다는 한계가 있다.\n",
        "    # 이 단점을 해결하는 목적으로 양방향 순한신경망(Bi-RNN)이 제안되었다. Bi-RNN은 기존의 순방향에 역방향을 추가하여,\n",
        "    # 은닉층에 추가하여 성능을 향상시켰다. 그러나 데이터 길이가 길고 층이 깊으면, 과거의 정보가 손실되는 단점이 있다.\n",
        "    # 이를 극복하기 위해 제안된 알고리즘이 양방향 LSTM이다\n",
        "\n",
        "    # 3.2 Hierarchical Decoder - Decoder에 계층적 2-Layer LSTM 신경망 적용\n",
        "    # 기존 Recurrent VAE의 decoder는 단순한 Stack RNN이다. 이 모델은 긴 Sequence에 대한 샘플링 결과가 좋지 않았고 원인은\n",
        "    # Output Sequence가 생성됨에 따라 Latent State가 힘을 잃기기 때문이라고 유추된다. 따라서 단방향 LSTM 계층을 추가하여\n",
        "    # 2계층 단방향 LSTM 신경망이 서로 종속적이고 계층적 구조를 갖게 하여 Latent State가 더 힘을 유지될 수 있게끔 해준다.\n",
        "\n",
        "    hparams=merge_hparams(\n",
        "        lstm_models.get_default_hparams(),\n",
        "        HParams(\n",
        "            # Hyper_Parameters에서 정의 - 모델의 파라미터\n",
        "            batch_size=BATCH_SIZE,\n",
        "            max_seq_len=MAX_SEQ_LEN,\n",
        "            z_size=Z_SIZE,\n",
        "            enc_rnn_size=ENC_RNN_SIZE,\n",
        "            dec_rnn_size=DEC_RNN_SIZE,\n",
        "            max_beta=MAX_BETA,\n",
        "            free_bits=FREE_BITS,\n",
        "            dropout_keep_prob=DROPOUT_KEEP_PROB,\n",
        "        )),\n",
        "    note_sequence_augmenter=None,\n",
        "    data_converter=data.GrooveConverter(\n",
        "        split_bars=4, steps_per_quarter=4, quarters_per_bar=4,\n",
        "        max_tensors_per_notesequence=20,\n",
        "        pitch_classes=data.ROLAND_DRUM_PITCH_CLASSES,\n",
        "        inference_pitch_classes=data.REDUCED_DRUM_PITCH_CLASSES),\n",
        "    tfds_name='groove/4bar-midionly',\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "SHPCckcpmKFi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train Model"
      ],
      "metadata": {
        "id": "Ri5BOfO1bP-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 학습(From music_vae_train.py)"
      ],
      "metadata": {
        "id": "m3nstYtbj0Lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# midi 데이터를 tensor로 변환\n",
        "def _get_input_tensors(dataset, config):\n",
        "  batch_size = config.hparams.batch_size\n",
        "  iterator = tf.data.make_one_shot_iterator(dataset)\n",
        "  (input_sequence, output_sequence, control_sequence,\n",
        "   sequence_length) = iterator.get_next()\n",
        "   # 데이터 로더\n",
        "  input_sequence.set_shape(\n",
        "      [batch_size, None, config.data_converter.input_depth])\n",
        "  output_sequence.set_shape(\n",
        "      [batch_size, None, config.data_converter.output_depth])\n",
        "  if not config.data_converter.control_depth:\n",
        "    control_sequence = None\n",
        "  else:\n",
        "    control_sequence.set_shape(\n",
        "        [batch_size, None, config.data_converter.control_depth])\n",
        "  sequence_length.set_shape([batch_size] + sequence_length.shape[1:].as_list())\n",
        "\n",
        "  return {\n",
        "      'input_sequence': input_sequence,\n",
        "      'output_sequence': output_sequence,\n",
        "      'control_sequence': control_sequence,\n",
        "      'sequence_length': sequence_length\n",
        "  }\n",
        "\n",
        "\n",
        "# 학습\n",
        "def train(train_dir,\n",
        "          config,\n",
        "          dataset_fn,\n",
        "          checkpoints_to_keep=5,\n",
        "          keep_checkpoint_every_n_hours=1,\n",
        "          num_steps=None,\n",
        "          master='',\n",
        "          num_sync_workers=0,\n",
        "          num_ps_tasks=0,\n",
        "          task=0):\n",
        "  \"\"\"Train loop.\"\"\"\n",
        "  tf.gfile.MakeDirs(train_dir)\n",
        "  is_chief = (task == 0)\n",
        "\n",
        "  with tf.Graph().as_default():\n",
        "    with tf.device(tf.train.replica_device_setter(\n",
        "        num_ps_tasks, merge_devices=True)):\n",
        "\n",
        "      model = config.model #config에서 설정한 모델 불러오기\n",
        "      model.build(config.hparams, # 파라미터 설정\n",
        "                  config.data_converter.output_depth,\n",
        "                  is_training=True)\n",
        "\n",
        "      optimizer = model.train(**_get_input_tensors(dataset_fn(), config))\n",
        "\n",
        "      hooks = []\n",
        "      if num_sync_workers:\n",
        "        optimizer = tf.train.SyncReplicasOptimizer(\n",
        "            optimizer,\n",
        "            num_sync_workers)\n",
        "        hooks.append(optimizer.make_session_run_hook(is_chief))\n",
        "\n",
        "      grads, var_list = list(zip(*optimizer.compute_gradients(model.loss)))\n",
        "      global_norm = tf.global_norm(grads)\n",
        "      tf.summary.scalar('global_norm', global_norm)\n",
        "\n",
        "      if config.hparams.clip_mode == 'value':\n",
        "        g = config.hparams.grad_clip\n",
        "        clipped_grads = [tf.clip_by_value(grad, -g, g) for grad in grads]\n",
        "      elif config.hparams.clip_mode == 'global_norm':\n",
        "        clipped_grads = tf.cond(\n",
        "            global_norm < config.hparams.grad_norm_clip_to_zero,\n",
        "            lambda: tf.clip_by_global_norm(  # pylint:disable=g-long-lambda\n",
        "                grads, config.hparams.grad_clip, use_norm=global_norm)[0],\n",
        "            lambda: [tf.zeros(tf.shape(g)) for g in grads])\n",
        "      else:\n",
        "        raise ValueError(\n",
        "            'Unknown clip_mode: {}'.format(config.hparams.clip_mode))\n",
        "      train_op = optimizer.apply_gradients(\n",
        "          list(zip(clipped_grads, var_list)),\n",
        "          global_step=model.global_step,\n",
        "          name='train_step')\n",
        "\n",
        "      logging_dict = {'global_step': model.global_step,\n",
        "                      'loss': model.loss}\n",
        "\n",
        "      hooks.append(tf.train.LoggingTensorHook(logging_dict, every_n_iter=100))\n",
        "      if num_steps:\n",
        "        hooks.append(tf.train.StopAtStepHook(last_step=num_steps))\n",
        "\n",
        "      scaffold = tf.train.Scaffold(\n",
        "          saver=tf.train.Saver(\n",
        "              max_to_keep=checkpoints_to_keep,\n",
        "              keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours))\n",
        "      tf_slim.training.train(\n",
        "          train_op=train_op,\n",
        "          logdir=train_dir,\n",
        "          scaffold=scaffold,\n",
        "          hooks=hooks,\n",
        "          save_checkpoint_secs=60,\n",
        "          master=master,\n",
        "          is_chief=is_chief)\n",
        "\n",
        "\n",
        "def run(config_map,\n",
        "        tf_file_reader=tf.data.TFRecordDataset,\n",
        "        file_reader=tf.python_io.tf_record_iterator):\n",
        "  \"\"\"Load model params, save config file and start trainer.\n",
        "  \"\"\"\n",
        "  config = config_map['groovae_4bar']\n",
        "  is_training = True\n",
        "\n",
        "  def dataset_fn():\n",
        "    return data.get_dataset(\n",
        "        config,\n",
        "        tf_file_reader=tf_file_reader,\n",
        "        is_training=True,\n",
        "        cache_dataset=True)\n",
        "\n",
        "  if is_training:\n",
        "    train(\n",
        "        train_dir = TRAIN_DIR,\n",
        "        config=config,\n",
        "        dataset_fn=dataset_fn,\n",
        "        num_steps=NUM_STEPS)\n",
        "  else:\n",
        "    print(\"EVAL\")"
      ],
      "metadata": {
        "id": "Iq4Ulk2bbQC7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run(CONFIG_MAP)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Kph94N4lbUl",
        "outputId": "639b736d-062e-4c1f-fcf8-c9ea455910d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/magenta/contrib/rnn.py:463: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/rnn.py:437: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "/usr/local/lib/python3.8/dist-packages/magenta/contrib/rnn.py:749: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "  self._kernel = self.add_variable(\n",
            "/usr/local/lib/python3.8/dist-packages/magenta/contrib/rnn.py:751: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "  self._bias = self.add_variable(\n",
            "/usr/local/lib/python3.8/dist-packages/magenta/models/music_vae/base_model.py:195: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  mu = tf.layers.dense(\n",
            "/usr/local/lib/python3.8/dist-packages/magenta/models/music_vae/base_model.py:200: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  sigma = tf.layers.dense(\n",
            "/usr/local/lib/python3.8/dist-packages/magenta/models/music_vae/lstm_utils.py:94: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  tf.layers.dense(\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/training_util.py:396: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py:1066: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Generate"
      ],
      "metadata": {
        "id": "DXmsmTI_bW3K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J-aAC_oNbaZz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}